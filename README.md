# Semantic Understanding of Indoor Spaces with Speech
**Overview**
This project attempts tp tackles the challenge of indoor navigation and interaction for individuals who suffer from vision and hearing problems.
By utilizing an Intel RealSense D435i and pretrained YOLO and CLIP models, our system is capable of predicting the semantic understanding of the
scene in addition to articulating that to the user with minimal delay.

**Tip:** 
If possible, use Windows, as the RealSense camera ran with less issues compared to macOS on our end.

**Project Code:**
Located under Final.py file

**Authors**
- Ankita Mahajan (ankitamj@umich.edu)
- Peter Zhao (peata@umich.edu)

